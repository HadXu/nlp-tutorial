#Python之天龙八部词云生成分析#

----------
今天接触到了一个中文分词神奇--结巴分词。做自然语言处理的同志们应该都接触过这个强大的分词神器。本人基于该分词神器以及word_cloud做了一个天龙八部词云分析。

直接上代码：

    #coding:utf-8
    
    import jieba
    from wordcloud import WordCloud 
    
    f = open(u'天龙八部.txt','r').read()
    s = {}
    f = jieba.cut(f)
    for w in f:
    	if len(w) > 1:
    		previous_count = s.get(w,0)
    		s[w] = previous_count+1
    
    word = sorted(s.items(),key=lambda (word,count):count, reverse = True)
    word = word[1:100]
    #print word[:100]
    wordcloud = WordCloud(font_path = 'MSYH.TTF').fit_words(word)
    import matplotlib.pyplot as plt
    plt.imshow(wordcloud) 
    plt.axis("off")
    plt.show()

最终的结果是：

![](http://i.imgur.com/q88KC81.png)

